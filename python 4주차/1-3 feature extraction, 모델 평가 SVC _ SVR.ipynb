{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction :FA / PCA/ MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FA 요인 분석 (Factor Analysis) 이란?\n",
    "\n",
    "요인분석에 대해서 먼저 간단하게 설명해보면,\n",
    "요인분석은 수많은 변수들 중에서 잠재된 몇 개의 변수(요인)을 찾아내는 것이다. \n",
    "\n",
    "출처: https://ai-times.tistory.com/112 [ai-times]\n",
    "        \n",
    "    학생들의 시험 성적 데이터를 예를 들어 생각해보자.\n",
    "    수학, 과학, 영어, 중국어, 독어, 작곡, 연주 의 점수(0점-100점)으로 구성, \n",
    "    수학, 과학은 상관관계가 있을 것이고 (수리계산능력) \n",
    "    영어, 중국어, 독어 가 상관관계가 있을 것이고 (외국어능력) \n",
    "    작곡, 연주 가 상관관계가 있을 것이다. (음악적능력, 음악적재능)\n",
    "    (위의 가정이 좀 이상할 수 있지만, 그냥 그렇다고 받아들이자...)\n",
    "\n",
    "즉, 원래 7개의 변수(과목)으로 구성되어있지만, \n",
    "내부적으로는 3개의 잠재변수 즉, [수리계산능력], [외국어능력], [음악적재능] 으로\n",
    "구성되었다고 가정한다. \n",
    "\n",
    "- 많은 수(7개)의 변수들을 소수의 몇 개의(3개)의 잠재된 변수로 찾아내는 것\n",
    " - 요인분석이라고 한다.요인분석은 데이터 축소(Data Reduction)과 관계가 있다.\n",
    "\n",
    "    이렇게 찾은 잠재변수를 영어로는 Latent Variable 이라고 부른다. \n",
    "\n",
    "이제 정리해보자.\n",
    "\n",
    "요인분석(Factor Analysis)은 변수들 간의 상관관계를 고려하여 \n",
    "저변에 내재된 개념인 요인들을 추출해내는 분석방법이다. \n",
    "다른 말로 하면, 요인분석은 변수들 간의 상관관계를 고려하여\n",
    "서로 유사한 변수들 끼리 묶어주는 방법이다. \n",
    "또 다른 말로 하면, 많은 변수로 구성된 데이터가 몇 개의 요인에 의해 \n",
    "영향을 받는가를 알아보는 것이라고도 할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA(Principal Component Analysis)란?\n",
    "\n",
    "차원축소(dimensionality reduction)와 변수추출(feature extraction) 기법으로 \n",
    "널리 쓰이고 있는 주성분분석(Principal Component Analysis)\n",
    "\n",
    "\n",
    "- PCA는 분포된 데이터들의 주성분(Principal Component)를 찾아주는 방법. \n",
    "    좀더 구체적으로 보면 \n",
    "    2차원 좌표평면에 n개의 점 데이터 (x1,y1), (x2,y2), ..., (xn,yn)들이 \n",
    "    타원형으로 분포되어 있을 때\n",
    "\n",
    "    이 데이터들의 분포 특성을 2개의 벡터로 가장 잘 설명할 수 있는 방법은\n",
    "    e1, e2 두 개의 벡터로 데이터 분포를 설명하는 것\n",
    "    e1의 방향과 크기, 그리고 e2의 방향과 크기를 알면\n",
    "    이 데이터 분포가 .어떤 형태인지를 \n",
    "    가장 단순하면서도 효과적으로 파악할 수 있다.\n",
    "\n",
    "    \n",
    "- PCA는 데이터 하나 하나에 대한 성분을 분석하는 것이 아니라, \n",
    "- 여러 데이터들이 모여 하나의 분포를 이룰 때 그 하나의 분포의 전체변수의 기여도\n",
    "- 그것이 즉, 변수들의 기여도조합:주성분을 분석하는 방법이다.\n",
    "    \n",
    "    주성분이라 함은 그 방향으로 데이터들의 분산이 가장 큰 방향벡터를 의미\n",
    "    \n",
    "    https://darkpgmr.tistory.com/110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다차원 척도법(MultiDimensional Scaling)\n",
    "\n",
    "- 다차원 관측값 또는 개체들 간의 거리(distance) 또는\n",
    "- 비유사성(dissimilarity)을 이용하여 개체들을 \n",
    "- 원래의 차원보다 낮은 차원(보통 2차원)의 공간상에 위치시켜\n",
    "- (spatial configuration) 개체들 사이의 구조 또는 관계를 쉽게 파악하고자 함\n",
    "\n",
    "즉, 차원의 축소를 통해 \n",
    "- 개체들의 상대적 위치 등을 통해 개체들 사이의 관계를 쉽게 파악하고자 하는데 목적이 있다\n",
    "\n",
    "- 공간적 배열에 대한 주관적인 해석에 중점을 두고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction 특성 추출  .text => CountVectorizer\n",
    "\n",
    "특징추출에서 CountVectorizer => 단어를 세서 텍스트마이닝 하는게 vectorizing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evil</th>\n",
       "      <th>horizon</th>\n",
       "      <th>of</th>\n",
       "      <th>problem</th>\n",
       "      <th>queen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   evil  horizon  of  problem  queen\n",
       "0     1        0   1        1      0\n",
       "1     1        0   0        0      1\n",
       "2     0        1   0        1      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ['problem of evil', #중복되지 않은 단어 5개 'problem 만 중복'\n",
    "         'evil queen',\n",
    "         'horizon problem']\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec=CountVectorizer() #텍스트마이닝 하는 vec 오브젝트 인스턴스\n",
    "                                  # text mining => vectorizing\n",
    "X=vec.fit_transform(sample) \n",
    "# 샘플데이터를 가져야서 각 행별 모든 단어별로 몇개 있는지 확인해줌\n",
    "\n",
    "\n",
    "print(type(X))\n",
    "# 배열 => 데이터프레임으로 만들어줌\n",
    "# 각 각의 행벡터는 하나의 책, 책별 분류\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction 특성 추출  .text => Tfidf \n",
    "\n",
    "=>Term frequency( 단어빈도 ) + inverse document frequency( 역문서 빈도)\n",
    "##### 자주 등장하지 않는 단어 => 중요한 단어에 가산점\n",
    "\n",
    "\n",
    "역문서빈도: 여러문서에 자주 등장하는 단어는 중요문서가 아니다. \n",
    "    단어빈도 = (단어수/전체문서) < 1 => 즉 단어비율이 매우 적음\n",
    "    \n",
    "    역문서= (전체문서/단어수) 분모분자 뒤집음::: 수가 커질것.\n",
    "    역문서값은 => 값이 너무 커지니까 :::로그처리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evil</th>\n",
       "      <th>horizon</th>\n",
       "      <th>of</th>\n",
       "      <th>problem</th>\n",
       "      <th>queen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680919</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       evil   horizon        of   problem     queen\n",
       "0  0.517856  0.000000  0.680919  0.517856  0.000000\n",
       "1  0.605349  0.000000  0.000000  0.000000  0.795961\n",
       "2  0.000000  0.795961  0.000000  0.605349  0.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec= TfidfVectorizer() #텍스트마이닝 하는 vec 오브젝트 인스턴스\n",
    "                                   # text mining => vectorizing\n",
    "X = vec.fit_transform(sample)\n",
    "#  evil 중복되서 2번째행에서 빈도값이 더 높아? 중요하지 않은건가 싶어서....??\n",
    "# of 중요하지 않은 단어가 역문서빈도가 높음\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score # 정확하게 맞춘 관측치 갯수 확인\n",
    "y_pred=[0,2,1,3]\n",
    "y_true=[0,1,2,3]\n",
    "print(accuracy_score(y_true, y_pred, normalize=True) )\n",
    "#정확도 0.5 normalize =>정규분포 0~1\n",
    "# default =true 0.5 값이 나옴\n",
    "\n",
    "accuracy_score(y_true, y_pred, normalize=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측모델과 분류모델 평가방법이 다름\n",
    " kernal :rbf(방사형 커널), poly, sigmoid( 0~1 ):로지스틱회귀분석은 시그모이드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support vector machine 데이터분포의 가장외곽의끝점에서 선을 그리면 convex hall \n",
    "# 끝점을 방향성 벡터로 만든게 support vector =>고차원에 용이함\n",
    "# 뭔말인지 이해 안되도 일단 머리속으로만 생각할것\n",
    "\n",
    "from sklearn import svm, datasets #SVM 속한 SVC 로딩 \n",
    "from sklearn.model_selection import cross_val_score \n",
    "                    #cross validation 교차검증 => model selection 전처리 1단계\n",
    "    \n",
    "iris =datasets.load_iris() #sklearn 패키지.dataset에\n",
    "X, y = iris.data, iris.target #독립변수, 종속변수 지정\n",
    "\n",
    "#gamma = 커널의 모양 통제\n",
    "clf = svm.SVC(gamma='scale', random_state=0)\n",
    "cross_val_score(clf, X, y, scoring='recall_macro', cv=5)\n",
    "#score recall:민감도로 측정 cv=: 5 그룹 나눠라 / 4개는 훈련용, 1개는 테스트용\n",
    "\n",
    "# 5번의 학습 = 차가 많으면 안된다.\n",
    "# 모델을 CVS 하면 :나눠진 그룹별 데이터: 결과 5개\n",
    "#                           => 데이터가 일반화 될수 있게 cross validation 수행!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류성능 평가 \n",
    "https://datascienceschool.net/view-notebook/731e0d2ef52c41c686ba53dcaf346f32/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true =[2, 0, 2, 2, 0, 1]\n",
    "y_pred =[0, 0, 2, 2, 0 ,2]\n",
    "\n",
    "confusion_matrix(y_true, y_pred)  #혼동행렬\n",
    "#    0, 1, 2    #y_true 0 일때 y_pred 0 인 값 수가 몇개인지 count\n",
    "# 0  2  0  0    첫행은 실제로 0인 두개의 데이터가 둘다 정확하게 0으로 예측\n",
    "# 1  0  0  1    두번째행은 실제로 1인 데이터가 예측시 2로 분류되었다는 뜻\n",
    "# 2  1  0  2    마지막행에서 실제로 2인 데이터 3개중 1개만 0으로 분류되었다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대각선이 정분류율\n",
    "\n",
    "# 이진 분류시스템의 예 \n",
    "#제품을 생산하는 제조공장에서는 완성된 제품에 대해 품질 테스트를 실시하여 \n",
    "#불량품을 찾아내고 찾아낸 불량품은 공장으로 되돌린다(리콜, recall)\n",
    "#암(cancer, 악성)을 검진할 때도 암에 걸린 것을 양성(陽性, positive)이라 하고\n",
    "#걸리지 않은 것을 음성이라고 한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "y=np.array([1,1,1,1,0,0]) # 실제값\n",
    "p=np.array([1,1,0,0,0,0]) # 예측값\n",
    "\n",
    "accuracy = np.mean(np.equal(y,p)) #같은값이면 정확도가 높지\n",
    "print(accuracy)                      # 총 6개중에 4개 같음 2/3 확률  \n",
    "\n",
    "#수업때 배운 정분류율  (TP + TN)/ (TP + TN + FP + FN)\n",
    "\n",
    "right =np.sum(y * p ==1) # 1이면서 같은것\n",
    "right # sum() 조건에 맞는 데이터 확인 = > 2개밖에 없지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_점수 값은  0.6666666666666666\n",
      "\n",
      "accuracy 정확도 0.6666666666666666\n",
      "precision 정밀도  1.0\n",
      "recall 민감도 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision=right/np.sum(p) # 정밀도 = TP/(TP+FP) # 분모가 예측값p 기준\n",
    "recall =right/np.sum(y)   # 민감도 = TP/(TP+TN) # 분모가 실제값y 기준\n",
    "\n",
    "\n",
    "f1_score=2 * precision*recall/(precision+recall)\n",
    "print(\"f1_점수 값은 \",f1_score)\n",
    "print()\n",
    "print('accuracy 정확도', metrics.accuracy_score(y,p))\n",
    "print('precision 정밀도 ', metrics.precision_score(y,p))\n",
    "print('recall 민감도', metrics.recall_score(y,p))\n",
    "\n",
    "metrics.f1_score(y,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         2\n",
      "           1       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.75      0.75      0.67         6\n",
      "weighted avg       0.83      0.67      0.67         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 분류의 평가 = 정밀도 민감도,  f1-score \n",
    "\n",
    "print(metrics.classification_report(y,p)) #로 모두 볼수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y,p)) \n",
    "# 혼동 행렬\n",
    "#  실제값          True     False\n",
    "# 예측값 positive    2        0\n",
    "#        negative    2        2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_classification = 분류용 가상 데이터 생성\n",
    "Scikit-Learn 패키지는 분류(classification) 모형의 테스트를 위해 \n",
    "여러가지 가상 데이터를 생성하는 함수를 제공한다.\n",
    "\n",
    "https://taeguu.tistory.com/15 # 예제로 연습해도 좋을듯\n",
    "https://datascienceschool.net/view-notebook/ec26c797cec646e295d737c522733b15/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류 성능평가\n",
    "사이킷런 패키지는 metrics 서브패키지에서 다음처럼 다양한 분류용 성능평가 명령을 제공한다.\n",
    "\n",
    "- confusion_matrix(y_true, y_pred)\n",
    "- accuracy_score(y_true, y_pred)\n",
    "- precision_score(y_true, y_pred)\n",
    "- recall_score(y_true, y_pred)\n",
    "\n",
    "     - 아직 안배움 fbeta_score(y_true, y_pred, beta)\n",
    "- f1_score(y_true, y_pred)\n",
    "- classfication_report(y_true, y_pred)\n",
    "     - 아직 안배움 roc_curve\n",
    "     - 아직 안배움 auc\n",
    "\n",
    "https://datascienceschool.net/view-notebook/731e0d2ef52c41c686ba53dcaf346f32/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.03418291, -0.38437236],\n",
       "       [ 4.06377686,  0.17863836],\n",
       "       [ 0.41966783, -1.38206096],\n",
       "       [-1.27225991,  0.6600493 ],\n",
       "       [-0.81664689,  1.16942291]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문제\n",
    "from sklearn.datasets import make_classification # 분류용 가상데이터 만들기\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X,y = make_classification(n_samples=16, n_features=2, n_informative=2, \n",
    "                          n_redundant=0, random_state=0)\n",
    "# X 는 독립변수 2개 (종속변수에 영향을 끼치는 변수가 2개인 )\n",
    "# 총 16개의 samples X\n",
    "# 종속변수는 0,1 로 이뤄진 y\n",
    "\n",
    "print(X.shape)\n",
    "X[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_02\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1)LogisticRegression을 실시하시오\n",
    "# 2)훈련데이터를 예측하시요\n",
    "\n",
    "model =LogisticRegression() # 인스턴스 하고\n",
    "model.fit(X, y)# fitting 계수완성\n",
    "\n",
    "data= LogisticRegression().fit(X, y) # 간단하게 그냥 한줄로 정의\n",
    "print(data)\n",
    "data.score(X,y) #???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3)결과를 평가하시요\n",
    "# 답\n",
    "y_hat = model.predict(X) # 훈련데이터로 예측값을 만들어주고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y,y_hat) # 정밀도 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y, y_hat) #민감도 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999999999999999"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82         8\n",
      "           1       0.86      0.75      0.80         8\n",
      "\n",
      "    accuracy                           0.81        16\n",
      "   macro avg       0.82      0.81      0.81        16\n",
      "weighted avg       0.82      0.81      0.81        16\n",
      "\n",
      "\n",
      "[[6 2]\n",
      " [1 7]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y, y_hat))\n",
    "print()\n",
    "print(metrics.confusion_matrix(y, y_hat, labels=[1,0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
